{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: implicit in /mnt/workspace/.local/lib/python3.5/site-packages (0.3.8)\n",
      "Requirement already satisfied: tqdm in /mnt/workspace/.local/lib/python3.5/site-packages (from implicit) (4.31.1)\n",
      "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.5/dist-packages (from implicit) (1.1.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (from implicit) (1.14.5)\n",
      "\u001b[33mYou are using pip version 18.0, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install the library we need for the notebook\n",
    "!pip3 install --user 'implicit'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Building a music recommender system (Spring 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As its name implies, a recommender system is a tool that helps predicting what a user may or may not like among a list of given items. In some sense, you can view this as an alternative to content search, as recommendation engines help users discover products or content that they may not come across otherwise. For example, Facebook suggests friends and pages to users. Youtube recommends videos which users may be interested in. Amazon suggests the products which users may need... Recommendation engines engage users to services, can be seen as a revenue optimization process, and in general help maintaining interest in a service.\n",
    "\n",
    "In this notebook, we study how to build a simple recommender system: we focus on music recommendations, and we use a simple algorithm to predict which items users might like, that is called ALS, alternating least squares.\n",
    "\n",
    "## Goals\n",
    "\n",
    "In this lecture, we expect students to:\n",
    "\n",
    "- Revisit (or learn) recommender algorithms\n",
    "\n",
    "- Understand the idea of Matrix Factorization and the ALS algorithm\n",
    "\n",
    "- Build a simple model for a real usecase: music recommender system\n",
    "\n",
    "- Understand how to validate the results\n",
    "\n",
    "## Steps\n",
    "\n",
    "We assume students to work outside lab hours on the learning material. These are the steps by which we guide students, during labs, to build a good basis for the end-to-end development of a recommender system:\n",
    "\n",
    "* Inspect the data using Pandas, and build some basic, but very valuable knowledge about the information we have at hand\n",
    "* Formally define what is a sensible algorithm to achieve our goal: given the \"history\" of user taste for music, recommend new music to discover. Essentialy, we want to build a statistical model of user preferences such that we can use it to \"predict\" which additional music the user could like\n",
    "* With our formal definition at hand, we will learn different ways to implement such an algorithm. Our goal here is to illustrate what are the difficulties to overcome when implementing an algorithm like Matrix Factorization\n",
    "* Finally, we will focus on an existing implementation, available in ```Implicit```, which we will use out of the box to build a reliable statistical model\n",
    "\n",
    "Now, you may think at this point we will be done!\n",
    "\n",
    "Well, you'd better think twice: one important topic we will cover in all our Notebooks is **how to validate the results we obtain**, and **how to choose good parameters to train models** especially when using an \"opaque\" library for doing the job. As a consequence, we will focus on the statistical validation of our recommender system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Data\n",
    "\n",
    "Understanding data is one of the most important part when designing any machine learning algorithm. In this notebook, we will use a data set published by Audioscrobbler - a music recommendation system for last.fm. Audioscrobbler is also one of the first internet streaming radio sites, founded in 2002. It provided an open API for scrobbling, or recording listeners' plays of artists' songs. last.fm used this information to build a powerful music recommender engine.\n",
    "\n",
    "## 1.1. Data schema\n",
    "\n",
    "Unlike a rating dataset which contains information about users' preference for products (one star, 3 stars, and so on), the datasets from Audioscrobbler only has information about events: specifically, it keeps track of how many times a user played songs of a given artist and the names of artists. That means it carries less information than a rating: in the literature, this is called explicit vs. implicit ratings.\n",
    "\n",
    "### Reading material\n",
    "\n",
    "- [Implicit Feedback for Inferring User Preference: A Bibliography](http://people.csail.mit.edu/teevan/work/publications/papers/sigir-forum03.pdf)\n",
    "- [Comparing explicit and implicit feedback techniques for web retrieval: TREC-10 interactive track report](http://trec.nist.gov/pubs/trec10/papers/glasgow.pdf)\n",
    "- [Probabilistic Models for Data Combination in Recommender Systems](http://mlg.eng.cam.ac.uk/pub/pdf/WilGha08.pdf)\n",
    "\n",
    "The data we use in this Notebook is available in 3 files (these files are stored in our HDFS layer, in the directory  ```/datasets/lastfm```):\n",
    "\n",
    "- **`user_artist_data.txt`**: It contains about 140,000+ unique users, and 1.6 million unique artists. About 24.2 million users' plays of artists' are recorded, along with their count. It has 3 columns separated by spaces: \n",
    "\n",
    "| UserID | ArtistID | PlayCount |\n",
    "|----|----|----|\n",
    "| ...|...|...|\n",
    "\n",
    "\n",
    "- **`artist_data.txt`** : It prodives the names of each artist by their IDs. It has 2 columns separated by tab characters (`\\t`).\n",
    "\n",
    "| ArtistID | Name |\n",
    "|---|---|\n",
    "|...|...|\n",
    "\n",
    "- **`artist_alias.txt`**: Note that when plays are scrobbled, the client application submits the name of the artist being played. This name could be misspelled or nonstandard. For example, \"The Smiths\", \"Smiths, The\", and \"the smiths\" may appear as distinct artist IDs in the data set, even though they are plainly the same. `artist_alias.txt` maps artist IDs that are known misspellings or variants to the canonical ID of that artist. The data in this file has 2 columns separated by tab characters (`\\t`).\n",
    "\n",
    "| MisspelledArtistID | StandardArtistID |\n",
    "|---|---|\n",
    "|...|...|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2. Understanding data: simple descriptive statistic\n",
    "\n",
    "In order to choose or design a suitable algorithm for achieving our goals, given the data we have, we should first understand data characteristics. To start, we import the necessary packages to work with regular expressions, Data Frames, and other nice features of our programming environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import matplotlib\n",
    "import implicit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"   # Required by implicit\n",
    "\n",
    "base = \"/mnt/datasets/lastfm/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 1\n",
    "\n",
    "#### Question 1.0 (Non-grading)\n",
    "\n",
    "Using Pandas, load data from `/datasets/lastfm/user_artist_data.txt` (407 MB) and show the first 20 entries (via function `show()`).\n",
    "\n",
    "```bash \n",
    "$ du -sh /mnt/datasets/lastfm/user_artist_data.txt\n",
    "407M     /mnt/datasets/lastfm/user_artist_data.txt\n",
    "```\n",
    "\n",
    "For this Notebook, from a programming point of view, we are given the schema for the data we use, which is as follows:\n",
    "\n",
    "```\n",
    "userID: long int\n",
    "artistID: long int\n",
    "playCount: int\n",
    "```\n",
    "\n",
    "Each line of the dataset contains the above three fields, separated by a \"white space\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>artistID</th>\n",
       "      <th>playCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000006</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000007</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000009</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000010</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000013</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000014</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000017</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000024</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000025</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000028</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000031</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000033</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000042</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000045</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000054</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000055</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000056</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000059</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1000002</td>\n",
       "      <td>1000062</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     userID  artistID  playCount\n",
       "0   1000002         1         55\n",
       "1   1000002   1000006         33\n",
       "2   1000002   1000007          8\n",
       "3   1000002   1000009        144\n",
       "4   1000002   1000010        314\n",
       "5   1000002   1000013          8\n",
       "6   1000002   1000014         42\n",
       "7   1000002   1000017         69\n",
       "8   1000002   1000024        329\n",
       "9   1000002   1000025          1\n",
       "10  1000002   1000028         17\n",
       "11  1000002   1000031         47\n",
       "12  1000002   1000033         15\n",
       "13  1000002   1000042          1\n",
       "14  1000002   1000045          1\n",
       "15  1000002   1000054          2\n",
       "16  1000002   1000055         25\n",
       "17  1000002   1000056          4\n",
       "18  1000002   1000059          2\n",
       "19  1000002   1000062         71"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the .csv file (might take a while)\n",
    "userArtistDF = pd.read_csv(base + 'user_artist_data.txt', \n",
    "                           sep=' ', \n",
    "                           header=None, \n",
    "                           names=['userID', 'artistID', 'playCount'])\n",
    "\n",
    "# Cast each column to its correct datatype\n",
    "userArtistDF['userID'] = userArtistDF['userID'].astype(np.int64, errors='ignore')\n",
    "userArtistDF['artistID'] = userArtistDF['artistID'].astype(np.int64, errors='ignore')\n",
    "userArtistDF['playCount'] = userArtistDF['playCount'].astype(np.int32, errors='ignore')\n",
    "\n",
    "userArtistDF[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 1.1: \n",
    "<div class=\"alert alert-info\">\n",
    "How many distinct users do we have in our data? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total n. of users:  148111\n"
     ]
    }
   ],
   "source": [
    "uniqueUsers = userArtistDF['userID'].nunique()\n",
    "print(\"Total n. of users: \", uniqueUsers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 1.2\n",
    "<div class=\"alert alert-info\">\n",
    "How many distinct artists do we have in our data ?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total n. of artists:  1631028\n"
     ]
    }
   ],
   "source": [
    "uniqueArtists = userArtistDF['artistID'].nunique()\n",
    "print(\"Total n. of artists: \", uniqueArtists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We just discovered that we have a total of 148,111 users in our dataset. Similarly, we have a total of 1,631,028 artists in our dataset. \n",
    "\n",
    "One thing we can see here is that Pandas provides very concise and powerful methods for data analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next, we might want to understand better user activity and artist popularity.\n",
    "\n",
    "Here is a list of simple descriptive queries that helps us reaching these purposes:\n",
    "\n",
    "* How many times each user has played a song? This is a good indicator of who are the most active users of our service. Note that a very active user with many play counts does not necessarily mean that the user is also \"curious\"! Indeed, she could have played the same song several times.\n",
    "* How many play counts for each artist? This is a good indicator of the artist popularity. Since we do not have time information associated to our data, we can only build a, e.g., top-10 ranking of the most popular artists in the dataset. Later in the notebook, we will learn that our dataset has a very \"loose\" definition about artists: very often artist IDs point to song titles as well. This means we have to be careful when establishing popular artists. Indeed, artists whose data is \"well formed\" will have the correct number of play counts associated to them. Instead, artists that appear mixed with song titles may see their play counts \"diluted\" across their songs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 2\n",
    "\n",
    "#### Question 2.1\n",
    "<div class=\"alert alert-info\">\n",
    "How many times each user has played a song? Show 5 samples of the result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>playCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>116</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>120</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>128</td>\n",
       "      <td>133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userID  playCount\n",
       "0      90        537\n",
       "1      98        186\n",
       "2     116         20\n",
       "3     120         65\n",
       "4     128        133"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute user activity\n",
    "# We are interested in how many playcounts each user has scored.\n",
    "userActivity = userArtistDF.groupby(['userID'], as_index=False)['playCount'].sum()\n",
    "userActivity[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 2.2\n",
    "<div class=\"alert alert-info\">\n",
    "Plot CDF (or ECDF) of the number of play counts per User ID.  \n",
    "\n",
    "Explain and comment the figure you just created:   \n",
    "<ul>\n",
    "<li>for example, look at important percentiles (10%, 25%, median, 75%, tails such as >90%) and cross check with what you have found above to figure out if the result is plausible. </li>\n",
    "<li>discuss about your users, with respect to the application domain we target in the notebook: you will notice that for some users, there is very little interaction with the system, which means that maybe reccommending something to them is going to be more difficult than for other users who interact more with the system. </li>\n",
    "<li>look at outliers and reason about their impact on your reccommender algorithm</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Ellipsis",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1b673f26d5ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plot the cumulative distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0muserActivity\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Calculate and plot the 5 important percentiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: Ellipsis"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the cumulative distribution\n",
    "userActivity[...].plot(..., ax=ax)\n",
    "\n",
    "# Calculate and plot the 5 important percentiles\n",
    "for i, quantile in enumerate([.1, 0.25, .5, .75, .9]):\n",
    "    ax.axvline(userActivity[...].quantile(quantile), label='%.0fth perc.' % (quantile*100))\n",
    "\n",
    "ax.semilogx()\n",
    "ax.set_title('ECDF of number of play counts per User ID')\n",
    "ax.set_xlabel('Play Counts')\n",
    "ax.set_ylabel('ECDF')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">PUT YOUR COMMENT HERE </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 2.3\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "How many play counts for each artist? Plot CDF or ECDF of the result.  \n",
    "\n",
    "Similarly to the previous question, you need to comment and interpret your result: what is the figure telling you?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "# Compute artist popularity\n",
    "# We are interested in how many playcounts per artist\n",
    "# ATTENTION! Grouping by artistID may be problematic, as stated above.\n",
    "\n",
    "artistPopularity = userArtistDF...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "...\n",
    "\n",
    "ax.semilogx()\n",
    "ax.set_title('ECDF of number of play counts per Artist ID')\n",
    "ax.set_xlabel('Play Counts')\n",
    "ax.set_ylabel('ECDF')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">PUT YOUR COMMENT HERE </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 2.4\n",
    "<div class=\"alert alert-info\">\n",
    "Plot a bar chart to show top 10 artists In terms of absolute play counts.  \n",
    "\n",
    "Comment the figure you just obtained: \n",
    "<ul>\n",
    "<li>are these reasonable results?</li>\n",
    "<li>is looking at top-10 artists enough to learn more about your data?</li>\n",
    "<li>do you see anything strange in the data?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "topN = 10\n",
    "topNArtist = ...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "topNArtist.plot(...,\n",
    "                ax=ax)\n",
    "\n",
    "ax.set_xlabel('Play Count')\n",
    "ax.set_ylabel('Artist')\n",
    "ax.set_title('Top-10 Artist ID per play counts')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All seems clear right now, but ... wait a second! What about the problems indicated above about artist \"disambiguation\"? Are these artist ID we are using referring to unique artists? How can we make sure that such \"opaque\" identifiers point to different bands? Let's try to use some additional dataset to answer this question:  `artist_data.txt` dataset. This time, the schema of the dataset consists in:\n",
    "\n",
    "```\n",
    "artist ID: long int\n",
    "name: string\n",
    "```\n",
    "\n",
    "We will try to find whether a single artist has two different IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 3\n",
    "\n",
    "#### Question 3.1\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Load the data ('\\t'-separated) from `/datasets/lastfm/artist_data.txt` and use the Pandas API to show 5 samples.  \n",
    "</div>\n",
    "\n",
    "**HINT**: `artist_data.txt` has malformed entries. If you encounter some error when parsing lines in data because of invalid entries, functions `pandas.DataFrame.dropna()` and `pandas.to_numeric()` will help you to eliminate these entries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "# Read from the .csv file\n",
    "artistDF = pd.read_csv(...,\n",
    "                       names=['artistID', 'name'])\n",
    "\n",
    "# Cast each column to its datatype\n",
    "...\n",
    "\n",
    "# Filter out possible malformed entries\n",
    "...\n",
    "\n",
    "artistDF[:5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 3.2\n",
    "<div class=\"alert alert-info\">\n",
    "Find 20 artists whose name contains `Aerosmith`. Take a look at artists that have ID equal to `1000010` and `2082323`. In your opinion, are they pointing to the same artist?  \n",
    "</div>\n",
    "\n",
    "**HINT**: Function `contains(string)` can be useful in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "# get artists whose name contains \"Aerosmith\"\n",
    "artistDF[....str.contains('Aerosmith')][:20]\n",
    "\n",
    "# show two examples\n",
    "artistDF[artistDF['artistID'] == 1000010]\n",
    "artistDF[artistDF['artistID'] == 2082323]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question correctly, we need to use an additional dataset `artist_alias.txt` which contains the ids of mispelled artists and standard artists. The schema of the dataset consists in:\n",
    "\n",
    "```\n",
    "mispelledID ID: long int\n",
    "standard ID: long int\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 3.3\n",
    "<div class=\"alert alert-info\">\n",
    "Using Pandas, load the dataset from `/datasets/lastfm/artist_alias.txt` then show 5 samples.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "artistAliasDF = pd.read_csv(...,\n",
    "                            names=['misspelledArtistID', 'standardArtistID'])\n",
    "...\n",
    "\n",
    "artistAliasDF[:5]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 3.4\n",
    "<div class=\"alert alert-info\">\n",
    "Verify the answer of question 3.2 (\"Are artists that have ID equal to `1000010` and `2082323` the same ?\") by finding the standard ids corresponding to the mispelled ids `1000010` and `2082323` respectively.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "artistAliasDF[ ... ]\n",
    "artistAliasDF[ ... ]\n",
    "\n",
    "# 1000010 is a standard id, so it haven't been considered as mispelled id in the dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 4\n",
    "\n",
    "The mispelled or nonstandard information about artist make our results in the previous queries a bit \"sloppy\". To overcome this problem, we can replace all mispelled artist ids by the corresponding standard ids and re-compute the basic descriptive statistics on the \"amended\" data.\n",
    "\n",
    "#### Question 4.2\n",
    "<div class=\"alert alert-info\">\n",
    "Replace the non-standard artist ids in the dataframe that was loaded from `/datasets/lastfm/user_artist_data.txt` by the corresponding standard ids then show 5 samples.\n",
    "</div>\n",
    "\n",
    "\n",
    "**NOTE 1**: If an id doesn't exist in the dictionary as a mispelled id, it is really a standard id.\n",
    "\n",
    "**NOTE 2**: be careful! you need to be able to verify that you indeed solved the problem of having bad artist IDs. In principle, for the new data to be correct, we should to have duplicate pairs (user, artist), potentially with different play counts, right? In answering the question, please **show** that you indeed fixed the problem and find a way to fix the duplicates (user, artist).\n",
    "\n",
    "**HINT**: have a look to the functions `pandas.merge()` and `pandas.Dataframe.combine_first()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "# Join the two dataframes based on the misspelledArtistID\n",
    "newUserArtistDF = pd.merge(userArtistDF, artistAliasDF, left_on=..., right_on=..., how=...)\n",
    "\n",
    "# Get the final realArtistID\n",
    "newUserArtistDF['realArtistID'] = newUserArtistDF[...].combine_first(newUserArtistDF[...]).astype(np.int64)\n",
    "\n",
    "# Delete columns not required\n",
    "newUserArtistDF.drop(['artistID', 'standardArtistID', 'misspelledArtistID'], axis=1, inplace=True)\n",
    "\n",
    "# Fix duplicates of the pair (userID, realArtistID)\n",
    "newUserArtistDF = newUserArtistDF.groupby(..., as_index=False)...\n",
    "\n",
    "print('Cleaning the dataset took %.2f seconds' % (time() - t0))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (newUserArtistDF.groupby(['userID', 'realArtistID'], as_index=False).count()['playCount'] == 1).all(),  \\\n",
    "    'Each combination user/artist should appear only once'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 5\n",
    "\n",
    "Well, our data frame contains clean and \"standard\" data. We can use it to redo previous statistic queries. Now that we have all data, including the relation (`artistID`/`name`), please find a way to use the artist name instead of its ID in all required plots.\n",
    "\n",
    "#### Question 5.1\n",
    "<div class=\"alert alert-info\">\n",
    "How many unique artists? Compare with the result when using old data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "uniqueArtists = newUserArtistDF...\n",
    "\n",
    "print(\"Total n. of artists: \", uniqueArtists)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 5.2\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Who are the top-10 artistis?\n",
    "<ul>\n",
    "  <li>In terms of absolute play counts</li>\n",
    "  <li>In terms of \"audience size\", that is, how many users listened to one of their track at least once</li>\n",
    "</ul>  \n",
    "\n",
    "Plot the results, and explain the figures you obtain.\n",
    "<div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "# calculate top-10 artists in term of play counts\n",
    "topN = 10\n",
    "\n",
    "artistPopularity = newUserArtistDF.groupby(...)...\n",
    "\n",
    "# Get the name of the artists based on their IDs\n",
    "artistPopularity = pd.merge(artistPopularity, ..., right_on=..., left_on=..., how=...)\n",
    "topNArtist = artistPopularity.sort_values(..., ascending=False)[:topN]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "topNArtist[...].plot(kind='barh', ax=ax)\n",
    "\n",
    "ax.set_yticklabels(topNArtist['name'])\n",
    "ax.set_xlabel('Play Count')\n",
    "ax.set_title('Top-10 Artist per play counts')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "# calculate top-10 artists in term of audience size\n",
    "\n",
    "topN = 10\n",
    "\n",
    "artistAudience = newUserArtistDF.groupby(...)...\n",
    "artistAudience = pd.merge(artistAudience, ..., right_on=..., left_on=..., how=...)\n",
    "topNArtist = artistAudience.sort_values(..., ascending=False)[:topN]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "topNArtist[...].plot(kind='barh', ax=ax)\n",
    "\n",
    "ax.set_yticklabels(topNArtist['name'])\n",
    "ax.set_xlabel('Play Count')\n",
    "ax.set_title('Top-10 Artist per play counts')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 5.3\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Who are the top-10 users?\n",
    "<ul>\n",
    "  <li>In terms of absolute play counts</li>\n",
    "  <li>In terms of \"curiosity\", that is, how many different artists they listened to</li>\n",
    "\n",
    "</ul>  \n",
    "\n",
    "Plot the results\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "# calculate top-10 user in term of play counts\n",
    "topN = 10\n",
    "\n",
    "userPlayCount = newUserArtistDF.groupby(..., as_index=False)...\n",
    "topNUser = userPlayCount.sort_values(...)...\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "topNUser[...].plot(kind='barh', ax=ax)\n",
    "\n",
    "ax.set_yticklabels(topNUser['userID'])\n",
    "ax.set_xlabel('Play Count')\n",
    "ax.set_title('Top-10 User per play count')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "# calculate top-10 user in term of curiosity\n",
    "topN = 10\n",
    "\n",
    "userCuriosity = ....\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "topNUser[...].plot(kind='barh', ax=ax)\n",
    "\n",
    "ax.set_yticklabels(...)\n",
    "ax.set_xlabel('Play Count')\n",
    "ax.set_title('Top-10 User per play count')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some valuable information about the data. It's the time to study how to build a statistical models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Build a statistical models to make recommendations\n",
    "\n",
    "## 2.1 Introduction to recommender systems\n",
    "\n",
    "In a recommendation-system application there are two classes of entities, which we shall refer to as `users` and `items`. Users have preferences for certain items, and these preferences must be inferred from the data. The data itself is represented as a `preference matrix` $A$, giving for each user-item pair, a value that represents what is known about the degree of preference of that user for that item. The table below is an example for a `preference matrix` of 5 users and `k` items. The `preference matrix` is also known as `utility matrix`.\n",
    "\n",
    "| .  | IT1 | IT2 | IT3 | ... | ITk |\n",
    "|----|---|---|---|---|---|\n",
    "| U1 | 1 | na | 5 | ... | 3 |\n",
    "| U2 | na | 2 | na | ... | 2 |\n",
    "| U3 | 5 | na | 3 | ... | na |\n",
    "| U4 | 3 | 3 | na | ... | 4 |\n",
    "| U5 | na | 1 | na | ... | ... |\n",
    "\n",
    "The value of row i, column j expresses how much does user `i` like item `j`. The values are often the rating scores of users for items. An unknown value implies that we have no explicit information about the user's preference for the item. The goal of a recommendation system is to predict \"the blanks\" in the `preference matrix`. For example, assume that the rating score is from 1 (dislike) to 5 (love), would user `U5` like `IT3` ? We have two approaches:\n",
    "\n",
    "* Designing our recommendation system to take into account properties of items such as brand, category, price... or even the similarity of their names. We can denote the similarity of items `IT2` and `IT3`, and then conclude that because user `U5` did not like `IT2`, they were unlikely to enjoy SW2 either.\n",
    "\n",
    "* We might observe that the people who rated both `IT2` and `IT3` tended to give them similar ratings. Thus, we could conclude that user `U5` would also give `IT3` a low rating, similar to `U5`'s rating of `IT2`\n",
    "\n",
    "It is not necessary to predict every blank entry in a `utility matrix`. Rather, it is only necessary to discover some entries in each row that are likely to be high. In most applications, the recommendation system does not oer users a ranking of all items, but rather suggests a few that the user should value highly. It may not even be necessary to nd all items with the highest expected ratings, but only to nd a large subset of those with the highest ratings.\n",
    "\n",
    "\n",
    "## 2.2 Families of recommender systems\n",
    "\n",
    "In general, recommender systems can be categorized into two groups:\n",
    "\n",
    "* **Content-Based** systems focus on properties of items. Similarity of items is determined by measuring the similarity in their properties.\n",
    "\n",
    "* **Collaborative-Filtering** systems focus on the relationship between users and items. Similarity of items is determined by the similarity of the ratings of those items by the users who have rated both items.\n",
    "\n",
    "In the usecase of this notebook, artists take the role of `items`, and `users` keep the same role as `users`.\n",
    "Since we have no information about `artists`, except their names, we cannot build a `content-based` recommender system.\n",
    "\n",
    "Therefore, in the rest of this notebook, we only focus on `Collaborative-Filtering` algorithms.\n",
    "\n",
    "## 2.3 Collaborative-Filtering \n",
    "In this section, we study a member of a broad class of algorithms called `latent-factor` models. They try to explain observed interactions between large numbers of users and products through a relatively small number of unobserved, underlying reasons. It is analogous to explaining why millions of people buy a particular few of thousands of possible albums by describing users and albums in terms of tastes for perhaps tens of genres, tastes which are **not directly observable or given** as data. \n",
    "\n",
    "First, we formulate the learning problem as a matrix completion problem. Then, we will use a type of `matrix factorization` model to \"fill in\" the blanks.  We are given implicit ratings that users have given certain items (that is, the number of times they played a particular artist) and our goal is to predict their ratings for the rest of the items. Formally, if there are $n$ users and $m$ items, we are given an $n \\times m$ matrix $R$ in which the generic entry $(u, i)$ represents the rating for item $i$ by user $u$. **Matrix $R$ has many missing entries indicating unobserved ratings, and our task is to estimate these unobserved ratings**.\n",
    "\n",
    "A popular approach to the matrix completion problem is **matrix factorization**, where we want to \"summarize\" users and items with their **latent factors**.\n",
    "\n",
    "### 2.3.1 Basic idea and an example of Matrix Factorization\n",
    "For example, given a preference matrix 5x5 as below, we want to approximate this matrix into the product of two smaller matrixes $X$ and $Y$ .\n",
    "\n",
    "$$\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    " 5 & 2 & 4 & 4 & 3 \\\\\n",
    " 3 & 1 & 2 & 4 & 1 \\\\\n",
    " 2 &  & 3 & 1 & 4 \\\\\n",
    " 2 & 5 & 4 & 3 & 5 \\\\\n",
    " 4 & 4 & 5 & 4 &  \\\\\n",
    "\\end{bmatrix}\n",
    "\\approx M^\\prime =\n",
    "\\begin{bmatrix}\n",
    " x_{11} & x_{12} \\\\\n",
    " x_{21} & x_{22} \\\\\n",
    " x_{31} & x_{32} \\\\\n",
    " x_{41} & x_{42} \\\\\n",
    " x_{51} & x_{52} \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    " y_{11} & y_{12} & y_{13} & y_{14} & y_{15} \\\\\n",
    " y_{21} & y_{22} & y_{23} & y_{24} & y_{25} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$M^\\prime$ is an approximation that is as close to A as possible. To calculate how far from $M$ $M^\\prime$ is, we often calculate the sum of squared distances of non-empty elements in $M$ and the corresponding elements in $M^\\prime$.\n",
    "In this way, for $M^\\prime$, besides the approximated elements in $M$, we also have the non-observed elements. Therefore, to see how much does user `i` like item `j`, we simply pick up the value of $M^\\prime_{i,j}$.\n",
    "\n",
    "The challenge is how to calculate $X$ and $Y$. The bad news is that this can't be solved directly for both the best $X$ and best $Y$ at the same time. Fortunately, if $Y$ is known, we can calculate the best of $X$, and vice versa. It means from the initial values of $X$ and $Y$ in the beginning, we calculate the best $X$ according to $Y$, and then calculate the best $Y$ according to the new $X$. This process is repeated until the distance from $XY$ to $M$ is small. It's simple, right ?\n",
    "\n",
    "Let's take an example. To compute the approximation for the above 5x5 matrix $M$, first, we initialize the value of $X$ and $Y$ as below.\n",
    "\n",
    "$$\n",
    "M^\\prime = X \\times Y =\n",
    "\\begin{bmatrix}\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With the initial iteration, we calculate the the Root-Mean-Square Error from $XY$ to $M$.\n",
    "\n",
    "Consider the rst rows of $M$ and $XY$ . We subtract the first row of $XY$ from the entries in the rst row of $M$, to get $3,0,2,2,1$. We square and sum these to get $18$. \n",
    "\n",
    "In the second row, we do the same to get $1,1,0,2,1$, square and sum to get $7$. \n",
    "\n",
    "In the third row, the second column is blank, so that entry is ignored when computing the RMSE. The dierences are $0,1,1,2$ and the sum of squares is $6$. \n",
    "\n",
    "For the fourth row, the dierences are $0,3,2,1,3$ and the sum of squares is $23$. \n",
    "\n",
    "The fth row has a blank entry in the last column, so the dierences are $2,2,3,2$ and the sum of squares is $21$. \n",
    "\n",
    "When we sum the sums from each of the ve rows, we get $18+7+6+23+21 = 75$. So, $RMSE=\\sqrt{75/23}=1.806$ where $23$ is the number of non-empty values in $M$.\n",
    "\n",
    "Next, with the given value of $Y$, we calculate $X$ by finding the best value for $X_{11}$.\n",
    "\n",
    "$$\n",
    "M^\\prime = X \\times Y =\n",
    "\\begin{bmatrix}\n",
    " x & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " x+1 & x+1 & x+1 & x+1 & x+1 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, to minimize the $RMSE$  we minimize the difference of the first rows $(5(x+1))^2 + (2(x+1))^2 + (4(x+1))^2 + (4(x+1))^2 + (3(x+1))^2$. By  taking the derivative and set that equal to 0, we pick $x=2.6$\n",
    "\n",
    "Given the new value of $X$, we can calculate the best value for $Y$.\n",
    "\n",
    "$$\n",
    "M^\\prime = X \\times Y =\n",
    "\\begin{bmatrix}\n",
    " 2.6 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    " 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    " y & 1 & 1 & 1 & 1 \\\\\n",
    " 1 & 1 & 1 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    " 3.6 & 3.6 & 3.6 & 3.6 & 3.6 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    " 2 & 2 & 2 & 2 & 2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "By doing the same process as before, we can pick value for $y=1.617$. After that, we can check if the $RMSE$ is not converged, we continue to update $X$ by $Y$ and vice versa. In this example, for simple, we only update one element of each matrix in each iteration. In practice, we can update a full row or full matrix at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Matrix Factorization: Objective and ALS Algorithm\n",
    "\n",
    "More formally, in general, we select $k$ latent features, and describe each user $u$ with a $k-$dimensional vector $x_u$, and each item $i$ with a $k-$dimensional vector $y_i$.\n",
    "\n",
    "Then, to predict user $u$'s rating for item $i$, we do as follows: $ r_{ui} \\approx x_{u}^{T}y_i$.\n",
    "\n",
    "This can be put, more elegantly, in a matrix form. Let $x_1, \\cdots x_n \\in \\mathbb{R}^k$ be the factors for the users, and $y_1, \\cdots y_m \\in \\mathbb{R}^k$ the factors for the items. The $k \\times n$ user matrix $X$ and the $k \\times m$ item matrix $Y$ are then defined by:\n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    " |   &         & |  \\\\\n",
    "x_1  &  \\cdots & x_n\\\\\n",
    " |   &         & |  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    " |   &         & |  \\\\\n",
    "y_1  &  \\cdots & y_i\\\\\n",
    " |   &         & |  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Our goal is to estimate the complete ratings matrix $R \\approx X^{T} Y$. We can formulate this problem as an optimization problem in which we aim to minimize an objective function and find optimal $X$ and $Y$ . In particular, we aim to minimize the least squares error of the observed ratings (and regularize):\n",
    "\n",
    "$$\n",
    "\\min_{X,Y} \\sum_{r_{ui} \\text{observed}}(r_{ui} - x_{u}^{T}y_i)^2 + \\lambda \\left( \\sum_{u} \\|x_u\\|^2 + \\sum_{i} \\|y_i\\|^2 \\right) \n",
    "$$\n",
    "\n",
    "Notice that this objective is non-convex (because of the $x_{u}^{T} y_i$ term); in fact its NP-hard to optimize. Gradient descent can be used as an approximate approach here, however it turns out to be slow and costs lots of iterations. Note however, that if we fix the set of variables $X$ and treat them as constants, then the objective is a convex function of $Y$ and vice versa. Our approach will therefore be to fix $Y$ and optimize $X$, then fix $X$ and optimize $Y$, and repeat until convergence. This approach is known as **ALS (Alternating Least Squares)**. For our objective function, the alternating least squares algorithm can be expressed with this simple pseudo-code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Initialize** $X$, $Y$\n",
    "\n",
    "**while(convergence is not true) do**\n",
    "\n",
    "\n",
    "**for** $u = 1 \\cdots n$ **do**\n",
    "\n",
    "$x_u = \\left( \\sum_{r_ui \\in r_{u*}} y_i y_{i}^{T} + \\lambda I_k \\right)^{-1} \\sum_{r_ui \\in r_{u*}} r_{ui} y_i $ \n",
    "   \n",
    "**end for**\n",
    "\n",
    "**for** $u = 1 \\cdots n$ **do**\n",
    "\n",
    "$y_i = \\left( \\sum_{r_ui \\in r_{*i}} x_u x_{u}^{T} + \\lambda I_k \\right)^{-1} \\sum_{r_ui \\in r_{*i}} r_{ui} x_u $ \n",
    "   \n",
    "**end for**\n",
    "\n",
    "\n",
    "**end while**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single machine, we can analyze the computational cost of this algorithm. Updating each $x_u$ will cost $O(n_u k^2 + k^3)$, where $n_u$ is the number of items rated by user $u$, and similarly updating each $y_i$ will cost $O(n_i k^2 + k^3)$, where $n_i$ is the number of users that have rated item $i$.\n",
    "\n",
    "\n",
    "Once weve computed the matrices $X$ and $Y$, there are several ways compute a prediction. The first is to do what was discussed before, which is to simply predict $ r_{ui} \\approx x_{u}^{T}y_i$ for each user $u$ and item $i$. \n",
    "This approach will cost $O(nmk)$ if wed like to estimate every user-item pair. \n",
    "\n",
    "However, this approach is prohibitively expensive for most real-world datasets. A second (and more holistic) approach is to use the $x_u$ and $y_i$ as features in another learning algorithm, incorporating these features with others that are relevant to the prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further readings\n",
    "Other methods for matrix factorization include:\n",
    "\n",
    "* Low Rank Approximation and Regression in Input Sparsity Time, by Kenneth L. Clarkson, David P. Woodruff. http://arxiv.org/abs/1207.6365\n",
    "* Generalized Low Rank Models (GLRM), by Madeleine Udell, Corinne Horn, Reza Zadeh, Stephen Boyd. http://arxiv.org/abs/1410.0342\n",
    "* Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares, by Trevor Hastie, Rahul Mazumder, Jason D. Lee, Reza Zadeh . Statistics Department and ICME, Stanford University, 2014. http://stanford.edu/~rezab/papers/fastals.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Usecase : Music recommender system\n",
    "\n",
    "In this usecase, we use the data of users and artists in the previous sections to build a statistical model to recommend artists for users.\n",
    " \n",
    "## 3.1 Requirements\n",
    "According to the properties of data, we need to choose a recommender algorithm that is suitable for this implicit feedback data. It means that the algorithm should learn without access to user or artist attributes such as age, genre,.... Therefore, an algorithm of type `collaborative filtering` is the best choice.\n",
    "\n",
    "Second, in the data, there are some users that have listened to only 1 artist. We need an algorithm that might provide decent recommendations to even these users. After all, at some point, every user starts out with just one play at some point! \n",
    "\n",
    "Third, we need an algorithm that scales, both in its ability to build large models, and to create recommendations quickly.\n",
    "\n",
    "From these requirement, we can choose using ALS algorithm in [Implicit](https://implicit.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 6\n",
    "\n",
    "The library choosen for implementing ALS has a strict requirement in terms of data format. In particular `userID` and `artistID` should be both incremetally assigned starting from 0 (e.g. the first valid `artistID` is 90 and it has to correspond to 0). In these next questions we will try to fix this issue.\n",
    "\n",
    "#### Question 6.1\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Create a table with incremental ID for artists and print the first 15 entries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "artistMappingDF = pd.DataFrame()\n",
    "artistMappingDF['originalArtistID']  = ...\n",
    "artistMappingDF['incrementalArtistID'] = ...\n",
    "\n",
    "artistMappingDF[:15]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 6.2 \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Create a table with incremental ID for users and print the first 15 entries.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "userMappingDF = pd.DataFrame()\n",
    "userMappingDF['originalUserID'] = ...\n",
    "userMappingDF['incrementalUserID'] = ...\n",
    "\n",
    "userMappingDF[:15]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 6.3 \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Starting from the clean dataset that we already have, find a way to replace the original IDs with the new ones. Print the first 15 entries in this new table.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "rescaledUserArtistDF = pd.merge(newUserArtistDF, artistMappingDF, \n",
    "                                left_on=..., \n",
    "                                right_on=...).drop(..., axis=1)\n",
    "\n",
    "rescaledUserArtistDF = pd.merge(rescaledUserArtistDF, userMappingDF, \n",
    "                                left_on=...,\n",
    "                                right_on=...).drop(..., axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we don't need no more `userArtistDF` and `newUserArtistDF`. We can delete them and free some memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "del userArtistDF\n",
    "del newUserArtistDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.4 Training our statistical model\n",
    "To train a model using ALS, we must use a sparse matrix as an input. Implicit uses the class sparse matrix to support the construction of a distributed preference matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "The implicit library expects data as a item-user sparse matrix. Now we create two matricies, one for fitting the model (item-user) and one for recommendations (the transpose of item-user -- user-item)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "import implicit\n",
    "\n",
    "def to_sparse(userArtistDF):\n",
    "    playCount = userArtistDF['playCount'].astype(float)\n",
    "    artistID = userArtistDF['incrementalArtistID']\n",
    "    userID = userArtistDF['incrementalUserID']\n",
    "    sparse_item_user = sparse.csr_matrix((playCount, (artistID, userID)))\n",
    "\n",
    "    return sparse_item_user\n",
    "\n",
    "sparse_item_user = to_sparse(rescaledUserArtistDF)\n",
    "sparse_user_item = sparse_item_user.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 7.1\n",
    "<div class=\"alert alert-info\">\n",
    "Train a model trained by using `implicit.als.AlternatingLeastSquares()`.\n",
    "</div>\n",
    "\n",
    "Read **carefully** the documentation of this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use some additional parameters to adjust the quality of the model. For now, let's set \n",
    "\n",
    "- `factors = 10`\n",
    "- `iterations = 5`\n",
    "- `regularization = 0.01`\n",
    "- `alpha = 1.0`\n",
    "\n",
    "to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "factors = 10\n",
    "iterations = 5\n",
    "regularization = 0.01\n",
    "alpha = 1.0\n",
    "\n",
    "# Calculate the confidence matrix by multiplying the item/user matrix by our alpha value.\n",
    "data_conf = (... * alpha).astype('double')\n",
    "\n",
    "# Create the model\n",
    "model = implicit.als.AlternatingLeastSquares(...)\n",
    "\n",
    "#Fit the model\n",
    "model.fit(data_conf)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 7.5\n",
    "<div class=\"alert alert-info\">\n",
    "Print the user features for the first user\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "model.user_factors...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 8\n",
    "<div class=\"alert alert-info\">\n",
    "Show the top-5 artist names recommendated for a given user, for example: `2093760` (please, try also with different users!).\n",
    "</div>\n",
    "\n",
    "**HINT**: The recommendations can be given by function `recommend()`. These recommendations are only artist ids. You have to map them to artist names by using data in `artist_data.txt`. Remember that we have transformed our IDs, so what you need to do is `userID -> incrementalUserID -> recommend() -> incrementalArtistID -> artistID -> name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "# Retrieve the incrementalUserID from a userID\n",
    "def get_incrementalUserID(userID):\n",
    "    return ...\n",
    "\n",
    "# Retrieve the original userID from an incrementalUserID\n",
    "def get_userID(incrementalUserID):\n",
    "    return ...\n",
    "\n",
    "# Retrieve the incrementalArtistID from an artistID\n",
    "def get_incrementalArtistID(artisID):\n",
    "    return ...\n",
    "\n",
    "# Retrieve the original artistID from an incrementalArtistID\n",
    "def get_artistID(incrementalArtistID):\n",
    "    return ...\n",
    "\n",
    "# Retrive the artist name\n",
    "def get_artist_name(artistID):\n",
    "    return ...\n",
    "\n",
    "\n",
    "# Retrive the recommendations\n",
    "recommendations = model.recommend(...)\n",
    "\n",
    "# Print the recommended artists\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.5 Evaluating Recommendation Quality \n",
    "\n",
    "In this section, we study how to evaluate the quality of our model. It's hard to say how good the recommendations are.\n",
    "One of serveral methods approach to evaluate  a recommender based on its ability to rank good items (artists) high in a list of recommendations. The problem is how to define \"good artists\". Currently, by training all data, \"good artists\" is defined as \"artists the user has listened to\", and the recommender system has already received all of this information as input. It could trivially return the users previously-listened artists as top recommendations and score perfectly. Indeed, this is not useful, because the recommender's is used to recommend artists that the user has **never** listened to. \n",
    "\n",
    "To overcome that problem, we can hide the some of the artist play data and only use the rest to train model. Then, this held-out data can be interpreted as a collection of \"good\" recommendations for each user. The recommender is asked to rank all items in the model, and the rank of the held-out artists are examined. Ideally the recommender places all of them at or near the top of the list.\n",
    "\n",
    "The recommender's score can then be computed by comparing all held-out artists' ranks to the rest.  The fraction of pairs where the held-out artist is ranked higher is its score. 1.0 is perfect, 0.0 is the worst possible score, and 0.5 is the expected value achieved from randomly ranking artists. \n",
    "\n",
    "AUC(Area Under the Curve) can be used as a metric to evaluate model. It is also viewed as the probability that a randomly-chosen \"good\" artist ranks above a randomly-chosen \"bad\" artist.\n",
    "\n",
    "Next, we split the training data into 2 parts: `trainData` and `cvData` with ratio 0.9:0.1 respectively, where `trainData` is the dataset that will be used to train model. Then we write a function to calculate AUC to evaluate the quality of our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "#### Question 9.1\n",
    "<div class=\"alert alert-info\">\n",
    "Split the data into `trainData` and `cvData` with ratio 0.9:0.1 and use the first part to train a statistic model with:\n",
    "<ul>\n",
    "<li>`rank`=10</li>\n",
    "<li>`iterations`=5</li>\n",
    "<li>`lambda_`=0.01</li>\n",
    "<li>`alpha`=1.0</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "***Hint***: have a look to `train_test_split()` in `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "from sklearn.model_selection import ...\n",
    "\n",
    "# Create train/test split\n",
    "trainUserArtistDF, testUserArtistDF = ...\n",
    "\n",
    "# Transform to sparse matrix\n",
    "train_sparse_item_user = to_sparse(...)\n",
    "train_sparse_user_item = train_sparse_item_user.T\n",
    "\n",
    "# Compute the confidence matrix\n",
    "data_conf = sparse_item_user * ...\n",
    "\n",
    "# Create the model\n",
    "model = implicit.als.AlternatingLeastSquares(...)\n",
    "\n",
    "# Train\n",
    "model.fit(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Area under the ROC curve: a function to compute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def findNumElementsSmallerThan(arr, x, start=0):\n",
    "        left = start\n",
    "        right = len(arr) -1\n",
    "        # if x is bigger than the biggest element in arr\n",
    "        if start > right or x > arr[right]:\n",
    "            return right + 1\n",
    "        mid = -1\n",
    "        while left <= right:\n",
    "            mid = (left + right) // 2\n",
    "            if arr[mid] < x:\n",
    "                left = mid + 1\n",
    "            elif arr[mid] > x:\n",
    "                right = mid - 1\n",
    "            else:\n",
    "                while mid-1 >= start and arr[mid-1] == x:\n",
    "                    mid -= 1\n",
    "                return mid\n",
    "        return mid if arr[mid] > x else mid + 1\n",
    "\n",
    "def _compute_auc(incremental_user_id):\n",
    "    positive_products = testUserArtistDF[testUserArtistDF['incrementalUserID'] == incremental_user_id]['incrementalArtistID'].values\n",
    "    positive_ranking = model.rank_items(incremental_user_id, \n",
    "                                        train_sparse_user_item, \n",
    "                                        positive_products)\n",
    "    positive_ranking = list(map(lambda x: x[1], positive_ranking))\n",
    "    if len(positive_products) <= 1: return \n",
    "        \n",
    "    train_data_user = trainUserArtistDF[trainUserArtistDF['incrementalUserID'] == incremental_user_id]['incrementalArtistID'].values\n",
    "    \n",
    "    if len(train_data_user) < 1: return\n",
    "    negative_products = np.random.choice(train_data_user, len(positive_products))  \n",
    "    if len(negative_products) <= 1: return\n",
    "        \n",
    "    if len(negative_products) != len(positive_products): return \n",
    "    \n",
    "    negative_ranking = model.rank_items(incremental_user_id, \n",
    "                                        train_sparse_user_item, \n",
    "                                        negative_products)\n",
    "    \n",
    "    negative_ranking = list(map(lambda x: x[1], negative_ranking))\n",
    "   \n",
    "    correct = 0\n",
    "    total = len(positive_ranking)*len(negative_ranking)\n",
    "\n",
    "    for positive in positive_ranking:\n",
    "        correct += findNumElementsSmallerThan(negative_ranking, positive)\n",
    "    return float(correct) / total\n",
    "\n",
    "def compute_auc(test_users_ratio=1, parallelism=4):  \n",
    "    \"\"\"\n",
    "        Computes the AUC of a train/test split\n",
    "        Note:\n",
    "            It assumes `trainUserArtistDF`, `testUserArtistDf` and `model` to be defined \n",
    "            globally\n",
    "        Arguments:\n",
    "            test_users_ratio (float): Ratio of users to be tested (0, 1]\n",
    "    \"\"\"\n",
    "    if test_users_ratio <=0 or test_users_ratio > 1:\n",
    "        warnings.warn('Parameter test_users_ratio has to be in (0, 1]. Setting test_users_ratio=1...')\n",
    "        test_users_ratio = 1\n",
    "    \n",
    "    pool = Pool(parallelism)\n",
    "    roc_auc = []\n",
    "    list_of_users = testUserArtistDF['incrementalUserID'].unique()[::int(1/test_users_ratio)]\n",
    "    for res in tqdm(pool.imap_unordered(_compute_auc, list_of_users, 64), total=len(list_of_users)):\n",
    "        roc_auc.append(res)\n",
    "    \n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    return np.array(list(filter(None.__ne__, roc_auc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Question 9.2\n",
    "<div class=\"alert alert-info\">\n",
    "Using `compute_auc()`, compute the AUC of the trained model.\n",
    "</div>\n",
    "\n",
    "***Note***: `compute_auc()` is computational expensive. To test if everything works, try to first compute the AUC for a small subset of users (10%) and than move to the entire set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "roc_auc = ...\n",
    "print('Average AUC = %.4f' % roc_auc.mean())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 9.3 \n",
    "\n",
    "The average AUC doesn't give the full picture here. Try to plot the distribution and the cumulative distribution of the AUC for all the users. Can you comment on these plots? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "fig, (ax0, ax1) = plt.subplots(2,1)\n",
    "\n",
    "# Plot distribution\n",
    "ax0.hist(..., bins='auto', density=True, ...)\n",
    "\n",
    "# Plot cumulative distribution\n",
    "ax1.hist(..., bins='auto', density=True, cumulative=True, ...)\n",
    "\n",
    "# Plot a vertical line on the average (distribution plot)\n",
    "ax0.axvline(..., label='Average AUC = %.3f' % ...)\n",
    "\n",
    "# Plot some percentile on the cumulative distribution\n",
    "for i, quantile in enumerate([.05, 0.25, .5, .75, .95]):\n",
    "    ax1.axvline(np.percentile(...), label='%.0fth perc.' % (quantile*100))\n",
    "\n",
    "    \n",
    "ax0.set_title('Distribution of AUC for all the users')\n",
    "ax1.set_title('Cumulative distribution of AUC for all the users')\n",
    "    \n",
    "ax0.legend()\n",
    "ax1.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.6 Personalized recommendations with ALS\n",
    "\n",
    "In the previous section, we build our models with some given paramters without any knowledge about them. Actually, choosing the best parameters' values is very important. It can significantly affect the quality of models. Especially, with the current implementation of ALS in MLLIB, these parameters are not learned by the algorithm, and must be chosen by the caller. The following parameters should get consideration before training models:\n",
    "\n",
    "* `rank = 10`: the number of latent factors in the model, or equivalently, the number of columns $k$ in the user-feature and product-feature matrices. In non-trivial cases, this is also their rank. \n",
    "\n",
    "* `iterations = 5`: the number of iterations that the factorization runs. Instead of runing the algorithm until RMSE converged which actually takes very long time to finish with large datasets, we only let it run in a given number of iterations. More iterations take more time but may produce a better factorization.\n",
    "\n",
    "* `lambda_ = 0.01`: a standard overfitting parameter. Higher values resist overfitting, but values that are too high hurt the factorization's accuracy.\n",
    "\n",
    "*  `alpha = 1.0`: controls the relative weight of observed versus unobserved userproduct interactions in the factorization. \n",
    "\n",
    "Although all of them have impact on the models' quality, `iterations` is more of a constraint on resources used in the factorization. So, `rank`, `lambda_` and `alpha` can be considered hyperparameters to the model. \n",
    "We will try to find \"good\" values for them. Indeed, the values of hyperparameter are not necessarily optimal. Choosing good hyperparameter values is a common problem in machine learning. The most basic way to choose values is to simply try combinations of values and evaluate a metric for each of them, and choose the combination that produces the best value of the metric. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 10\n",
    "\n",
    "#### Question 10.1\n",
    "<div class=\"alert alert-info\">\n",
    "For simplicity, assume that we want to explore the following parameter space: $ rank \\in \\{10, 50\\}$, $lambda\\_ \\in \\{1.0, 0.0001\\}$ and $alpha \\in \\{1.0, 40.0\\}$.\n",
    "\n",
    "Find the best combination of them in terms of the highest AUC value.\n",
    "</div>\n",
    "\n",
    "***Hint***: have a look to `ParameterGrid()` in `sklearn.model_selection` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "from sklearn.model_selection import ...\n",
    "\n",
    "# Define the hyperparameters\n",
    "hyperparameters = {\n",
    "    'factors': [10, 25, 50], \n",
    "    'regularization' : ..., \n",
    "    'alpha' : ...}\n",
    "\n",
    "# Compute all combinations\n",
    "parameter_grid = list(ParameterGrid(...))\n",
    "\n",
    "# Just a table to save the results\n",
    "resultsDF = pd.DataFrame(columns=['factors', 'regularization', 'alpha', 'auc'])\n",
    "\n",
    "for parameter_combination in parameter_grid:    \n",
    "    model = implicit.als.AlternatingLeastSquares(...)\n",
    "\n",
    "    train_data_conf = ....\n",
    "\n",
    "    model.fit(train_data_conf)\n",
    "    \n",
    "    auc = compute_auc(...)\n",
    "    \n",
    "    parameter_combination['auc'] = ...\n",
    "    resultsDF = resultsDF.append(parameter_combination, ignore_index=True)\n",
    "    \n",
    "resultsDF.sort_values(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Question 10.2\n",
    "\n",
    "Sometimes an exhaustive search in the hyperparameter space is a waste of time and resources.\n",
    "There are a lot of research questions open that deal with automating the end-to-end process of applying machine learning to real-world problems (e.g. **AutoML**).\n",
    "\n",
    "Random sampling in the hyperparameter space is a possible -- very simple but yet quite powerfull -- technique.\n",
    "\n",
    "Implement a simple parameter sampler and try a bunch of different configurations.\n",
    "\n",
    "***Hint***: have a look to `ParameterSampler()` in `sklearn.model_selection` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "import scipy.stats \n",
    "from sklearn.model_selection import ...\n",
    "\n",
    "# Hyperparameters are now definded by probability densities\n",
    "hyperparameters = {\n",
    "    'factors': scipy.stats.randint(10, 50), \n",
    "    'regularization' : scipy.stats.uniform(0.0001, 1.), \n",
    "    'alpha' : scipy.stats.uniform(1., 40.)}\n",
    "\n",
    "parameter_grid = list(ParameterSampler(...))\n",
    "\n",
    "# You can either append the results to the previous table or create a new one\n",
    "# resultsDF = pd.DataFrame(columns=['factors', 'regularization', 'alpha', 'auc'])\n",
    "\n",
    "for parameter_combination in parameter_grid:    \n",
    "    model = ...\n",
    "    \n",
    "    train_data_conf = ...\n",
    "\n",
    "    model.fit(...)\n",
    "    \n",
    "    auc = compute_auc(...)\n",
    "    \n",
    "    parameter_combination['auc'] = auc.mean()\n",
    "    resultsDF = resultsDF.append(parameter_combination, ignore_index=True)\n",
    " \n",
    "    \n",
    "resultsDF.sort_values(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Question 10.2 \n",
    "<div class=\"alert alert-info\">\n",
    "Using \"optimal\" hyper-parameters in question 10.1, re-train the model and show top-5 artist names recommendated for user `2093760`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "###################################################################\n",
    "####                      TO COMPLETE                         #####\n",
    "###################################################################\n",
    "\n",
    "model = ...\n",
    "\n",
    "train_data_conf = ...\n",
    "model.fit(...)\n",
    "\n",
    "user_id = 2093760\n",
    "for incremental_artist_id, score in model.recommend(...):\n",
    "    print(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, we introduce an algorithm to do matrix factorization and the way of using it to make recommendation. Further more, we studied how to build a large-scale recommender system using ALS algorithm and evaluate its quality. Finally, a simple approach to choose good parameters is mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- The example in section 2 is taken from [Recommender system](infolab.stanford.edu/~ullman/mmds/ch9.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "rise": {
   "scroll": true,
   "theme": "white",
   "transition": "zoom"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
